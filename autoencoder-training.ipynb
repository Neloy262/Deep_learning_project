{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12332732,"sourceType":"datasetVersion","datasetId":7774278}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install timm\n!pip install comet_ml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T05:15:22.855968Z","iopub.execute_input":"2025-07-03T05:15:22.856622Z","iopub.status.idle":"2025-07-03T05:16:48.802792Z","shell.execute_reply.started":"2025-07-03T05:15:22.856569Z","shell.execute_reply":"2025-07-03T05:16:48.801704Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport timm\n\n\n# --------------------------------------------\n# Residual Block with optional upsampling\n# --------------------------------------------\nclass ResidualBlockTranspose(nn.Module):\n    def __init__(self, in_channels, out_channels, upsample=False):\n        super().__init__()\n        self.upsample = upsample\n\n        self.conv1 = nn.ConvTranspose2d(\n            in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1\n        ) if upsample else nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        if upsample or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=1, stride=2, output_padding=1)\n                if upsample else nn.Conv2d(in_channels, out_channels, kernel_size=1),\n                nn.BatchNorm2d(out_channels)\n            )\n        else:\n            self.shortcut = nn.Identity()\n\n    def forward(self, x):\n        identity = self.shortcut(x)\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        return self.relu(out + identity)\n\n\n# --------------------------------------------\n# Encoder using timm (ResNet)\n# --------------------------------------------\nclass ResNetEncoder(nn.Module):\n    def __init__(self, model_name=\"resnet18\", pretrained=True):\n        super().__init__()\n        self.encoder = timm.create_model(model_name, pretrained=pretrained, features_only=True)\n        self.out_channels = self.encoder.feature_info[-1]['num_chs']\n\n    def forward(self, x):\n        # Only return last feature map\n        return self.encoder(x)[-1]  # e.g. shape (B, 512, 16, 16) for 512×512 input\n\n\n# --------------------------------------------\n# Decoder: Upsample back to 512×512\n# --------------------------------------------\nclass ResNetDecoder(nn.Module):\n    def __init__(self, in_channels, out_channels=3):\n        super().__init__()\n        self.decoder = nn.Sequential(\n            ResidualBlockTranspose(in_channels, 256, upsample=True),  # 16 → 32\n            ResidualBlockTranspose(256, 128, upsample=True),          # 32 → 64\n            ResidualBlockTranspose(128, 64, upsample=True),           # 64 → 128\n            ResidualBlockTranspose(64, 32, upsample=True),            # 128 → 256\n            ResidualBlockTranspose(32, 16, upsample=True),            # 256 → 512\n            nn.Conv2d(16, out_channels, kernel_size=3, padding=1),\n            nn.Sigmoid()  # assume input is in [0, 1]\n        )\n\n    def forward(self, x):\n        return self.decoder(x)\n\n\n# --------------------------------------------\n# Autoencoder model\n# --------------------------------------------\nclass ResNetAutoencoder(nn.Module):\n    def __init__(self, model_name=\"resnet18\"):\n        super().__init__()\n        self.encoder = ResNetEncoder(model_name)\n        self.decoder = ResNetDecoder(self.encoder.out_channels)\n\n    def forward(self, x):\n        z = self.encoder(x)\n        out = self.decoder(z)\n        return out\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T05:16:48.804673Z","iopub.execute_input":"2025-07-03T05:16:48.804980Z","iopub.status.idle":"2025-07-03T05:16:59.960962Z","shell.execute_reply.started":"2025-07-03T05:16:48.804953Z","shell.execute_reply":"2025-07-03T05:16:59.960189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image\nclass ImageDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        \"\"\"\n        Args:\n            root_dir (str): Path to folder containing images.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n        self.root_dir = root_dir\n        self.image_paths = [os.path.join(root_dir, fname) \n                            for fname in os.listdir(root_dir) \n                            if fname.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif'))]\n        \n        # self.image_paths = self.image_paths[0:201] \n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = Image.open(image_path).convert(\"RGB\")  # Ensure 3 channels\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image,\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T05:16:59.961771Z","iopub.execute_input":"2025-07-03T05:16:59.962268Z","iopub.status.idle":"2025-07-03T05:16:59.968553Z","shell.execute_reply.started":"2025-07-03T05:16:59.962242Z","shell.execute_reply":"2025-07-03T05:16:59.967779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_reconstructions(model, dataloader, device,epoch):\n    model.eval()\n    with torch.no_grad():\n        for images, _ in dataloader:\n            images = images.to(device)\n            outputs = model(images)\n            break\n\n    images = images.cpu().numpy()\n    outputs = outputs.cpu().numpy()\n\n    n = min(6, images.shape[0])\n    plt.figure(figsize=(12, 4))\n    for i in range(n):\n        plt.subplot(2, n, i + 1)\n        plt.imshow(images[i].transpose(1, 2, 0))\n        plt.title(\"Original\")\n        plt.axis(\"off\")\n\n        plt.subplot(2, n, i + n + 1)\n        plt.imshow(outputs[i].transpose(1, 2, 0))\n        plt.title(\"Reconstructed\")\n        plt.axis(\"off\")\n        \n    image_path = f\"/kaggle/working/reconstructed_image_{epoch}.png\"\n    plt.savefig(image_path)\n    return image_path\n    # experiment.log_image(image_path)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T05:16:59.970399Z","iopub.execute_input":"2025-07-03T05:16:59.970636Z","iopub.status.idle":"2025-07-03T05:16:59.993347Z","shell.execute_reply.started":"2025-07-03T05:16:59.970618Z","shell.execute_reply":"2025-07-03T05:16:59.992608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nimport os  # Assuming your model code is in model.py\nimport matplotlib.pyplot as plt\nfrom comet_ml import start\nfrom torch.utils.data import random_split\n# -----------------------------\n# Configuration\n# -----------------------------\nexperiment = start(\n  api_key=\"clEyXjBpSkvfrD5bYxCf3vVK9\",\n  project_name=\"diabetic-retinopathy_unsupervised\",\n  workspace=\"neloy-sarwar\",\n)\n\ndata_dir = \"/kaggle/input/diabetic-retinopathy-test-resized-512/test_images_resized_512\"  # should contain subfolders if using ImageFolder\nbatch_size = 32\nnum_epochs = 20\nlr = 3e-4\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nsave_dir = \"/kaggle/working/checkpoints\"\nos.makedirs(save_dir, exist_ok=True)\n\n# -----------------------------\n# Data transforms and loader\n# -----------------------------\ntransform = transforms.Compose([\n    # transforms.Resize((512, 512)),\n    transforms.ToTensor(),\n])\n\ndataset = ImageDataset(root_dir=data_dir, transform=transform)\n\n# Define split ratio\nval_split = 0.2  # 20% for validation\nval_size = int(len(dataset) * val_split)\ntrain_size = len(dataset) - val_size\n\n# Split dataset\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n# -----------------------------\n# Initialize model, loss, optimizer\n# -----------------------------\nmodel = ResNetAutoencoder(\"resnet18\").to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# -----------------------------\n# Training loop\n# -----------------------------\n# best_loss = float('inf')  # Initialize best loss\n\n# for epoch in range(num_epochs):\n#     model.train()\n#     running_loss = 0.0\n\n#     for images, _ in train_loader:\n#         images = images.to(device)\n\n#         # Forward pass\n#         outputs = model(images)\n#         loss = criterion(outputs, images)\n\n#         # Backward pass and optimization\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n#         print(f\"Epoch [{epoch+1}/{num_epochs}] Batch loss:{loss.item()}\")\n#         running_loss += loss.item() * images.size(0)\n\n#     epoch_loss = running_loss / len(train_loader.dataset)\n#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n#     experiment.log_metric(\"epoch_train_loss\", epoch_loss, step=epoch)\n#     try:\n#         # Save the best model\n#         if epoch_loss < best_loss:\n#             best_loss = epoch_loss\n#             torch.save({\n#                 'epoch': epoch,\n#                 'model_state_dict': model.state_dict(),\n#                 'optimizer_state_dict': optimizer.state_dict(),\n#                 'loss': best_loss\n#             }, os.path.join(save_dir, \"best_autoencoder.pt\"))\n#             print(f\"✅ Best model saved at epoch {epoch+1} with loss {best_loss:.4f}\")\n        \n#         # Show reconstructions every 5 epochs\n#         if (epoch + 1) % 5 == 0:\n#             image_path = show_reconstructions(model, val_loader, device,epoch)\n#             experiment.log_image(image_path)\n#     except:\n#         pass\n\nbest_loss = float('inf')\nepochs_no_improve = 0\npatience = 5  # stop after 5 non-improving epochs\nearly_stop = False\n\nfor epoch in range(num_epochs):\n    if early_stop:\n        break\n\n    model.train()\n    running_loss = 0.0\n\n    for images, _ in train_loader:\n        images = images.to(device)\n\n        outputs = model(images)\n        loss = criterion(outputs, images)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        print(f\"Epoch [{epoch+1}/{num_epochs}] Batch loss:{loss.item()}\")\n        running_loss += loss.item() * images.size(0)\n\n    epoch_loss = running_loss / len(train_loader.dataset)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n    experiment.log_metric(\"epoch_train_loss\", epoch_loss, step=epoch)\n\n    # Evaluate on validation set\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for images in val_loader:\n            if isinstance(images, (tuple, list)):\n                images = images[0]\n            images = images.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, images)\n            val_loss += loss.item() * images.size(0)\n\n    val_loss /= len(val_loader.dataset)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Val Loss: {val_loss:.4f}\")\n    experiment.log_metric(\"epoch_val_loss\", val_loss, step=epoch)\n\n    # Check for improvement\n    if val_loss < best_loss:\n        best_loss = val_loss\n        epochs_no_improve = 0\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': best_loss\n        }, os.path.join(save_dir, \"best_autoencoder.pt\"))\n        print(f\"✅ Best model saved at epoch {epoch+1} with val loss {best_loss:.4f}\")\n    else:\n        epochs_no_improve += 1\n        print(f\"⚠️ No improvement for {epochs_no_improve} epoch(s).\")\n\n    # Show reconstructions every 5 epochs\n    if (epoch + 1) % 5 == 0:\n        try:\n            image_path = show_reconstructions(model, val_loader, device, epoch)\n            experiment.log_image(image_path)\n        except:\n            pass\n\n    # Early stopping condition\n    if epochs_no_improve >= patience:\n        print(f\"⏹️ Early stopping triggered at epoch {epoch+1}\")\n        early_stop = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T05:16:59.994294Z","iopub.execute_input":"2025-07-03T05:16:59.994562Z","iopub.status.idle":"2025-07-03T05:20:19.562824Z","shell.execute_reply.started":"2025-07-03T05:16:59.994543Z","shell.execute_reply":"2025-07-03T05:20:19.561368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}